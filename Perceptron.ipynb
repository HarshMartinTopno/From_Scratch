{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshMartinTopno/From_Scratch/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qWyeOGZU9B8y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxJkQJ5Gm1W"
      },
      "source": [
        "The Perceptron Learning Rule is a simple algorithm used to train a perceptron, which is one of the earliest types of artificial neural networks.\n",
        "\n",
        "**Goal:**\n",
        "\n",
        "The perceptron aims to learn how to classify data points into one of two categories (e.g., \"yes\" or \"no,\" \"1\" or \"0\") by finding the best line (or hyperplane in higher dimensions) that separates the data.\n",
        "\n",
        "**Steps of the Perceptron Learning Rule\n",
        "Initialize Weights:**\n",
        "\n",
        "\n",
        "* **Initialize Weights:**\n",
        " Start with random or zero values for the weights and bias. These are parameters that will be adjusted to improve classification.\n",
        "\n",
        "* **Feed Input and Compute Output:**\n",
        "\n",
        "For a given input vector (*X*), compute the perceptron's output ($𝑦_{pred}$) using the formula:\n",
        "\n",
        "   * $y_{pred}$ = step(*W* . *X* + *b*)\n",
        "      * W: Weight vector\n",
        "      * X: Input vector.\n",
        "      * b: Bias.\n",
        "      * step: Activation function that outputs 1 if the result is positive and 0 otherwise.\n",
        "\n",
        "* **Compare Prediction with Actual Output:**\n",
        "\n",
        "  Check if the perceptron’s prediction matches the true label ($y_{true}$):\n",
        "    * If $y_{pred}$ == $y_{true}$ :\n",
        "      * True: No change to the weights or bias\n",
        "\n",
        "      * False: Update the weights and bias\n",
        "\n",
        "* **Update Weights and Bias:**\n",
        "\n",
        "  When the prediction is wrong, adjust the weights and bias using the rule:\n",
        "\n",
        " * $𝑊_{new} = 𝑊_{old} + 𝜂⋅( 𝑦_{true} - y_{pred}) . X$\n",
        "\n",
        " * $b_{new} = b_{old} + 𝜂 . (y_{true} - y_{pred}) $\n",
        "\n",
        "    * η: Learning rate (a small positive number that controls the step size of adjustments).\n",
        "\n",
        "    * $y_{true} - y_{pred}$ : The error term.\n",
        "\n",
        "* **Repeat for All Data Points:**\n",
        "\n",
        "  Go through all data points repeatedly until the perceptron classifies all points correctly (or reaches a stopping criterion, like a maximum number of iterations).\n",
        "\n",
        "\n",
        "* Key Points\n",
        "  * **Linear Separability:** The perceptron  only works if the data is linearly separable (i.e., you can draw a straight line to separate the categories).\n",
        "\n",
        "  * **Guaranteed Convergence:** If the data is linearly separable, the perceptron learning rule will eventually find the correct weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dvbaI7CH9dvl"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "\n",
        "  def __init__(self, eta = 0.01, n_iter = 50, random_state = 1):\n",
        "\n",
        "    self.eta = eta   # learning rate\n",
        "    self.n_iter = n_iter # number of epochs\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def fit(self, X, y):\n",
        "\n",
        "    rgen = np.random.RandomState(self.random_state)\n",
        "    # initial weight vector contains small random numbers drawn from a normal distribution with a standard deviation of 0.01\n",
        "    self.w_ = rgen.normal(loc = 0.0, scale = 0.01, size = X.shape[1])\n",
        "    self.b_ = np.float(0.) # bias with initial value zero\n",
        "    self.errors_ = []\n",
        "\n",
        "    for _ in range(self.n_iter):\n",
        "      errors = 0\n",
        "      for xi, target in zip(X,y):\n",
        "        update = self.eta * (target - self.predict(xi))\n",
        "        self.w_ += update * xi\n",
        "        self.b_ += update\n",
        "        errors += int(update != 0.0)\n",
        "      self.errors_.append(errors)\n",
        "    return self\n",
        "  def net_input(self, X):\n",
        "\n",
        "    return np.dot(X, self.w_) + self.b_\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    return np.where(self.net_input(X) >= 0.0, 1, 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMIoqVxggVMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}