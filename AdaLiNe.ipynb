{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4iOXdCaK8OdYNbL7qFYXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshMartinTopno/From_Scratch/blob/main/AdaLiNe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "gw7vxEUYe2_X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "gXphvLnPgIRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaline (ADAptive LInear NEuron) Algorithm\n",
        "\n",
        "Adaline is a single-layer neural network used for binary classification. It is an improvement over the perceptron algorithm and is based on the concept of minimizing a cost function using gradient descent. Below is an explanation of the algorithm based on the provided code implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## Algorithm Steps\n",
        "\n",
        "### 1. **Initialization**\n",
        "- The weights (`self.w_`) and bias (`self.b_`) are initialized randomly using a normal distribution with a small scale (`scale=0.01`).\n",
        "- The learning rate (`self.lr`) and number of iterations (`self.n_iters`) are set during initialization.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Training (Fit Method)**\n",
        "Adaline uses **gradient descent** to minimize the **mean squared error (MSE)** loss function. For each epoch (iteration):\n",
        "\n",
        "1. **Net Input**: Compute the net input as a linear combination of the input features and weights, plus the bias:\n",
        "\n",
        "   $net$\\_$input$ $=$ $X$ $⋅$ $w$ + $b$\n",
        "\n",
        "\n",
        "2. **Activation Function**: The activation function is the identity function (i.e., no transformation is applied):\n",
        "   \n",
        "   $output =$ $net$\\_$input$\n",
        "\n",
        "3. **Error Calculation**: Compute the error as the difference between the true labels (`y`) and the predicted output:\n",
        "   \n",
        "   $errors = y - output$\n",
        "\n",
        "4. **Weight and Bias Update**: Update the weights and bias using the gradient of the MSE loss function:\n",
        "\n",
        "   $w$ = $w$ + $η$ $⋅$ $\\frac{2}{N}$ $⋅$ $X^{T}$ $⋅$ $errors$\n",
        "\n",
        "   $b$ = $b$ + $η$ $⋅$ $\\frac{2}{N}$ $⋅$ $mean(errors)$\n",
        "\n",
        "   Here, $eta$ is the learning rate, and $N$ is the number of samples.\n",
        "\n",
        "\n",
        "5. **Loss Calculation**: Compute the MSE loss for the current epoch and store it in `self.losses_`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Prediction**\n",
        "- After training, the model predicts the class label for new input data:\n",
        "  - Compute the net input and apply the activation function.\n",
        "  - If the output is greater than or equal to 0.5, predict class 1; otherwise, predict class 0.\n",
        "\n",
        "---\n",
        "\n",
        "## Benefits of Adaline\n",
        "\n",
        "1. **Smooth Convergence**:\n",
        "   - Adaline uses gradient descent to minimize the MSE loss, which provides smoother convergence compared to the perceptron algorithm.\n",
        "\n",
        "2. **Continuous Optimization**:\n",
        "   - The use of a continuous loss function (MSE) allows for better optimization and fine-tuning of weights.\n",
        "\n",
        "3. **Interpretability**:\n",
        "   - The weights and bias learned by Adaline can be interpreted as the importance of each feature in the decision-making process.\n",
        "\n",
        "4. **Foundation for Advanced Models**:\n",
        "   - Adaline serves as a foundation for more advanced models like logistic regression and multi-layer neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## Drawbacks of Adaline\n",
        "\n",
        "1. **Sensitive to Learning Rate**:\n",
        "   - The performance of Adaline heavily depends on the choice of the learning rate. A too-small learning rate leads to slow convergence, while a too-large learning rate can cause instability.\n",
        "\n",
        "2. **Linear Separability**:\n",
        "   - Like the perceptron, Adaline works well only for linearly separable data. It cannot handle non-linear decision boundaries without feature transformations.\n",
        "\n",
        "3. **No Guarantee of Global Optimum**:\n",
        "   - Gradient descent can get stuck in local minima, especially if the loss surface is non-convex.\n",
        "\n",
        "4. **Scalability**:\n",
        "   - For large datasets, Adaline can be computationally expensive due to the need to compute the gradient over the entire dataset in each iteration.\n",
        "\n",
        "5. **Binary Classification Only**:\n",
        "   - The basic Adaline implementation is limited to binary classification tasks. Extending it to multi-class classification requires modifications.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Differences from Perceptron\n",
        "\n",
        "- **Loss Function**:\n",
        "  - Perceptron uses a step function for classification and updates weights based on misclassifications.\n",
        "  - Adaline uses a continuous loss function (MSE) and updates weights using gradient descent.\n",
        "\n",
        "- **Convergence**:\n",
        "  - Perceptron converges only if the data is linearly separable.\n",
        "  - Adaline converges to the best possible solution even if the data is not perfectly separable.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "075OMmNFi8p2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L4iRh8JKbn7x"
      },
      "outputs": [],
      "source": [
        "class Adaline:\n",
        "\n",
        "  \"\"\"ADAptive LInear NEuron classifier. (ADALINE)\n",
        "\n",
        "\n",
        "  Parameters\n",
        "  ------------\n",
        "\n",
        "  eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "\n",
        "  n_iter : int\n",
        "      Passes over the training dataset.\n",
        "\n",
        "  random_state : int\n",
        "      Random number generator seed for random weight initialization.\n",
        "\n",
        "\n",
        "  Attributes\n",
        "  -----------\n",
        "\n",
        "  w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "\n",
        "  b_ : Scalar\n",
        "      Bias unit after fitting.\n",
        "\n",
        "  losses_ : list\n",
        "      Mean squared error loss function values in each epoch.\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, lr = 0.01, n_iters = 50, random_state = 69):\n",
        "\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "\n",
        "  def fit(self, X, y):\n",
        "\n",
        "    rgen = np.random.RandomState(self.random_state)\n",
        "    self.w_ = rgen.normal(loc = 0.0, scale = 0.01, size = X.shape[1])\n",
        "    self.b_ = float(0.)\n",
        "    self.losses_ = []\n",
        "\n",
        "    for i in range(self.n_iters):\n",
        "\n",
        "      net_input = self.net_input(X)\n",
        "      output = self.activation(net_input)\n",
        "      errors = (y - output)\n",
        "      self.w_ += self.lr * 2.0 * X.T.dot(errors) / X.shape[0]\n",
        "      self.b_ += self.lr * 2.0 * errors.mean()\n",
        "      loss = (errors**2).mean()\n",
        "      self.losses_.appen(loss)\n",
        "    return self\n",
        "\n",
        "  def net_input(self, X):\n",
        "    return np.dot(X, self.w_) + self.b_\n",
        "\n",
        "\n",
        "  def activation(self, X):\n",
        "    return X\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yzt_OXTIevXK"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}